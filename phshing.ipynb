{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from transformers import (BloomTokenizerFast,\n",
    "                          BloomForTokenClassification,\n",
    "                          DataCollatorForTokenClassification, \n",
    "                          AutoModelForTokenClassification, \n",
    "                          TrainingArguments, Trainer)\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model_name = \"bloom-560m\"\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(f\"bigscience/{model_name}\", add_prefix_space=True)\n",
    "model = BloomForTokenClassification.from_pretrained(f\"bigscience/{model_name}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d52c7ce68c06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# datasets = load_dataset('conll2003')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all_emails.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(datasets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# datasets = load_dataset('conll2003')\n",
    "datasets = load_dataset('csv', data_files='all_emails.csv')\n",
    "#print(datasets)\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Splitting the dataset into 80% training and 20% testing\n",
    "train_test_split = datasets['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# The resulting splits are contained in a DatasetDict object\n",
    "splits = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "\n",
    "# Now you have splits['train'] and splits['test'] as your training and testing sets\n",
    "\n",
    "\n",
    "print(\"Dataset Object Type:\", type(datasets[\"train\"]))\n",
    "print(\"Training Examples:\", len(datasets[\"train\"]))\n",
    "\n",
    "datasets[\"train\"][100]\n",
    "\n",
    "# print(\"Dataset Object Type:\", type(datasets[\"train\"]))\n",
    "# print(\"Training Examples:\", len(datasets[\"train\"]))\n",
    "\n",
    "# datasets[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d78d5f0dfdea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# label_list = datasets[\"train\"].features[\"lebal\"].feature.names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# label_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# label_list = datasets[\"train\"].features[\"lebal\"].feature.names\n",
    "# label_list\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = datasets[\"train\"][0]\n",
    "print(example)\n",
    "tokenized_input = tokenizer(example[\"email\"])\n",
    "# tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tonkenize all input\n",
    "def tokenizeInputs(inputs):\n",
    "    \n",
    "#     tokenized_inputs = tokenizer(inputs[\"email\"], max_length = 512, truncation=True, is_split_into_words=True)\n",
    "#     word_ids = tokenized_inputs.word_ids()\n",
    "#     ner_tags = inputs[\"ner_tags\"]\n",
    "#     labels = [ner_tags[word_id] for word_id in word_ids]\n",
    "#     tokenized_inputs[\"labels\"] = labels\n",
    "    \n",
    "    tokenized_inputs = tokenizer(inputs[\"email\"], max_length = 512, truncation=True)\n",
    "    \n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    label = inputs[\"label\"]\n",
    "    labels = label\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = datasets[\"train\"][100]\n",
    "print(datasets[\"train\"][100][\"label\"])\n",
    "tokenizeInputs(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(tokenizeInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = 0\n",
    "for sample in tokenized_datasets[\"train\"]:\n",
    "    token_count = token_count + 1\n",
    "    \n",
    "print(\"Tokens in Training Set:\", token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets = tokenized_datasets.remove_columns([\"id\", \"tokens\", \"ner_tags\", \"pos_tags\", \"chunk_tags\"])\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"email\",\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(f\"bigscience/{model_name}\", num_labels=12).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parameters:\", model.num_parameters())\n",
    "print(\"Expected Input Dict:\", model.main_input_name )\n",
    "\n",
    "# Estimate FLOPS needed for one training example\n",
    "sample = tokenized_datasets[\"train\"][0]\n",
    "sample[\"input_ids\"] = torch.Tensor(sample[\"input_ids\"])\n",
    "flops_est = model.floating_point_ops(input_dict = sample, exclude_embeddings = False)\n",
    "\n",
    "print(\"FLOPS needed per Training Sample:\", flops_est )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    save_strategy= \"epoch\", # Disabled for runtime evaluation \n",
    "    evaluation_strategy=\"steps\", #\"steps\", # Disabled for runtime evaluation \n",
    "    eval_steps = 500,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    "    #fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-13035f89f08f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Eval Loss: {eval_results['eval_loss']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Eval Loss: {eval_results['eval_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned = BloomForTokenClassification.from_pretrained(\"./results/checkpoint-1172\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = datasets[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "\n",
    "id2label = {id : label for id, label in enumerate(label_names)}\n",
    "label2id = {label: id for id, label in enumerate(label_names)}\n",
    "\n",
    "model_tuned.config.id2label = id2label\n",
    "model_tuned.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tuned.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    \"HuggingFace is a company based in Paris and New York\", \n",
    "    add_special_tokens=False, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model_tuned(**inputs).logits\n",
    "\n",
    "predicted_token_class_ids = logits.argmax(-1)\n",
    "\n",
    "# Note that tokens are classified rather then input words which means that\n",
    "# there might be more predicted token classes than words.\n",
    "# Multiple token classes might account for the same word\n",
    "predicted_tokens_classes = [model_tuned.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
    "predicted_tokens_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BLOOM",
   "language": "python",
   "name": "bloom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
